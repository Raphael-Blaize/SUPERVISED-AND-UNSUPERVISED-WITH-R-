---
title: "UNSUPERVISED LEARNING WITH R"
by: "Raphael Blaize"
output: html_document
---



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
install.packages('contrib.url')

```

# PROBLEM DEFINITION

## a) Specifying the Question
Perform clustering and getting insights from the analysis and visualizations that will be done.

## b) Defining the metrics for success
Bivariates and univariates are two different types of variables. Analyzing preliminary data Make a clustering list based on the conclusions you reached from your research and visualizations. Provide comparisons between the techniques taught this week, e.g., K-Means clustering or Hierarchical clustering, after they've been implemented, highlighting the benefits and limits of each in the context of your analysis.

## c) Understanding the context
Kira Plastinina is a Russian brand that is sold through a defunct chain of retail stores in Russia, Ukraine, Kazakhstan, Belarus, China, Philippines, and Armenia. The brand’s Sales and Marketing team would like to understand their customer’s behavior from data that they have collected over the past year. More specifically, they would like to learn the characteristics of customer groups..

## d) Recording the Experimental Design
```
1. Define the question, the success metric, the setting, and the experimental design used.
2. Read and investigate the provided dataset.
3. Identify outliers, abnormalities, and missing data in the dataset and address them.
4. Conduct univariate and bivariate statistical analysis.
5. Create clusters based on the findings of your study and visualizations.
```

## Data Section


```{r}
#importing the Dataset and loading the libraries
shoppers = read.csv('online_shoppers_intention.csv')
shoppers
```

## CHECKING THE DATA

```{r}
#Checking the column names 
colnames(shoppers)
```
```{r}
#Checking datatypes of the dataset 
sapply(shoppers, class)
```


```{r}
# previewing the head of the dataset
head(shoppers, n = 5)
```

```{r}
# previewing the tail of the dataset
tail(shoppers, n = 5)
```
```{r}
# checking the structure of the data
str(shoppers)
```

## DATA CLEANING
```{r}
# checking for missing values
sum(is.na(shoppers))
```
```
There are 112 Missing values in the dataset we will proceed to drop them 

```
```{r}
# dropping missing values
df = na.omit(shoppers)
df
```

```{r}
# checking for duplicates
dim(df[duplicated(df),])[1]
```
```
There are 119 duplicates in the dataset, i will procced to drop them
```

```{r}
# dropping the duplicates
install.packages("tidyverse")
library(tidyverse)
df2 = df %>% distinct()
df2
```


```{r}
## Checking if the duplicates have been dropped
dim(df2[duplicated(df2),])[1]
```

```{r}
## Checking for outliers
df3 <- df2 %>% select_if(is.numeric)
boxplot(df3)
```
```
From the above we can see that most outliers lie in the Product related column i will not proceed to drop the outliers as they may have valuable information when it comes to our analysis
```



## EXPLORATORY DATA ANALYSIS

### Univariate Analysis

```{r}
## Checking summary of our data
summary(df2)
```

#### Histograms

```{r}
## Histograms
hist(df2$ProductRelated_Duration)
```

```{r}
## Histograms
hist(df2$ProductRelated)
```

```{r}
## Histograms
hist(df2$BounceRates)
```

```{r}
## Histograms
hist(df2$PageValues)
```


### Bivariate analysis

```{r}
# finding the covariance in the numerical variables
cov(df3)
```

```{r}
# finding the correlation coefficient
cor(df3)
```

```{r}
# VISULAISING THE CORRELATIONS
install.packages("corrplot")
library(corrplot)
corrplot(cor(df3))
```

```
From the above plot we can see that Administrative_Duration and Administrative seem to have a strong positive correlation of about 0.6
```

```{r}
library(ggplot2)
ggplot(data = df2, aes(x = ProductRelated_Duration, fill = Revenue))+
    geom_histogram(bins = 15,color = 'black',  alpha = 0.5) +
    labs(title = 'ProductRelated_Duration with Revenue', x = 'ProductRelated_Duration', y = 'Frequency', fill = 'Revenue') 
```

```{r}
library(ggplot2)
ggplot(data = df2, aes(x = ProductRelated, fill = Revenue))+
    geom_histogram(bins = 15,color = 'black',  alpha = 0.5) +
    labs(title = 'ProductRelated with Revenue', x = 'ProductRelated', y = 'Frequency', fill = 'Revenue') 
```


```{r}
library(ggplot2)
ggplot(data = df2, aes(x = Administrative, fill = Revenue))+
    geom_histogram(bins = 15,color = 'black',  alpha = 0.5) +
    labs(title = 'Administrative with Revenue', x = 'Administrative', y = 'Frequency', fill = 'Revenue') 
```





```{r}
library(ggplot2)
ggplot(data = df2, aes(x = Administrative_Duration, fill = Revenue))+
    geom_histogram(bins = 15,color = 'black',  alpha = 0.5) +
    labs(title = 'Administrative_Duration with Revenue', x = 'Administrative_Duration', y = 'Frequency', fill = 'Revenue') 
```


### IMPLEMENTATION OF THE SOLUTION

#### K-MEANS CLUSTERING

```{r}
# Normalizing the dataset so that no particular attribute 
# has more impact on clustering algorithm than others.
# ---
# 
normalize <- function(x){
  return ((x-min(x)) / (max(x)-min(x)))
}

## Chossing relevant columns to use 
df4 = subset(df2, select = c("ProductRelated", "ProductRelated_Duration", "PageValues"))
df4
```

```{r}
#Changing our categrocial variables to Numerical
#df5 = sapply(df4, unclass)
#df5
```

```{r}
df4$ProductRelated<- normalize(df4$ProductRelated)
df4$ProductRelated_Duration<- normalize(df4$ProductRelated_Duration)
df4$PageValues<- normalize(df4$PageValues)
head(df4)
```

```{r}
# Splitting the dataset into the Training set and Test set
# install.packages('caTools')
library(caTools)
set.seed(123)
split = sample.split(df4$PageValues, SplitRatio = 0.8)
training_set = subset(df4, split == TRUE)
test_set = subset(df4, split == FALSE)
```


```{r}
# Feature Scaling
training_set = scale(training_set)
test_set = scale(test_set)

# Using the elbow method to find the optimal number of clusters
set.seed(6)
wcss = vector()
for (i in 1:10) wcss[i] = sum(kmeans(df4, i)$withinss)
plot(1:10,
     wcss,
     type = 'b',
     main = paste('The Elbow Method'),
     xlab = 'Number of clusters',
     ylab = 'WCSS')
```

```
From the above we can see that the optimal number of clusters to use is round 3 so i will use the four clusters in the case of this dataset

```

```{r}
# Fitting K-Means to the dataset
set.seed(29)
kmeans = kmeans(x = dataset, centers = 4)
y_kmeans = kmeans$cluster
```

```{r}
# Visualising the clusters
library(cluster)
clusplot(dataset,
         y_kmeans,
         lines = 0,
         shade = TRUE,
         color = TRUE,
         labels = 2,
         plotchar = FALSE,
         span = TRUE,
         main = paste('Clusters of Pages'),
         xlab = 'ProductRelated',
         ylab = 'ProductRelatedDuration')
```

#### Hierarchical Clustering

```{r}

# Splitting the dataset into the Training set and Test set
set.seed(123)
split = sample.split(df4$PageValues, SplitRatio = 0.8)
training_set = subset(df4, split == TRUE)
test_set = subset(df4, split == FALSE)
```



```{r}
# Feature Scaling
training_set = scale(training_set)
test_set = scale(test_set)
```



```{r}
# Using the dendrogram to find the optimal number of clusters
dendrogram = hclust(d = dist(df4, method = 'euclidean'), method = 'ward.D')
plot(dendrogram,
     main = paste('Dendrogram'),
     xlab = 'Pages',
     ylab = 'Euclidean distances')
```


```{r}
# Fitting Hierarchical Clustering to the dataset
hc = hclust(d = dist(df4, method = 'euclidean'), method = 'ward.D')
y_hc = cutree(hc, 5)
```





